================================================================================
MASTER THESIS DEFENSE - SPEAKER SCRIPT
================================================================================
Total Duration: ~15 minutes (19 slides)
Timing guide per slide is marked with [TIME]
================================================================================


SLIDE 1: TITLE [0:00 - 0:30]
--------------------------------------------------------------------------------
Good morning/afternoon, esteemed committee members. My name is Yahia Amir Yahia
Gamal, and today I will present my master thesis on "Intelligent Analysis of
Automotive Sensor Faults Using Self-Supervised Learning and Real-Time
Simulation," supervised by Dr. Mohammad Abboush at the Institute of Software
and Systems Engineering.


SLIDE 2: OUTLINE [0:30 - 0:50]
--------------------------------------------------------------------------------
I have structured this presentation into ten parts. I will begin with the
motivation and problem statement, introduce the research questions, explain
why we chose SimCLR among other self-supervised methods, then present our
three-phase framework, the datasets, the training process, and finally the
comprehensive evaluation results. I will conclude with a summary and
directions for future work.


SLIDE 3: MOTIVATION [0:50 - 2:00]
--------------------------------------------------------------------------------
Modern vehicles are equipped with over a hundred sensors that feed into
safety-critical systems such as adaptive cruise control, lane keeping, and
autonomous emergency braking. When these sensors develop faults -- for
instance, a miscalibrated accelerator pedal sensor or a noisy speed sensor --
the consequences can be severe, potentially leading to incorrect vehicle
behavior.

The current state of the art in sensor fault detection relies on supervised
learning, which requires large labeled datasets containing examples of every
possible fault type and severity. Obtaining such labeled data is expensive,
requires specialized test setups, and is inherently incomplete -- we cannot
anticipate every possible failure mode.

Our approach takes a fundamentally different path: we train exclusively on
healthy driving data using self-supervised contrastive learning, specifically
the SimCLR framework. The system learns what "normal" looks like, and then
detects faults as deviations from this learned normality. No labeled fault
examples are needed at any stage.

As shown in the V-model on the right, our work operates at the Hardware-in-
the-Loop testing level, which is a critical step in automotive validation
according to ISO 26262.


SLIDE 4: RESEARCH QUESTIONS [2:00 - 2:45]
--------------------------------------------------------------------------------
This thesis addresses three specific research questions.

First, can self-supervised contrastive learning on healthy driving data produce
representations that effectively detect sensor faults when applied to HIL
simulation data? This tests the core feasibility of the approach.

Second, how does the detection threshold affect the trade-off between precision
and recall? We evaluate six percentile levels from the 15th to the 40th to
find the optimal operating point.

Third, which fault types and sensor locations are most and least detectable, and
can we explain the differences from a physical perspective? Understanding this
helps identify the strengths and limitations of the approach.


SLIDE 5: RELATED WORK - WHY SimCLR? [2:45 - 3:45]
--------------------------------------------------------------------------------
Before presenting our framework, let me explain why we chose SimCLR among
several self-supervised learning methods.

The table on the left compares five prominent SSL approaches. MoCo requires
a momentum encoder and a memory bank, adding architectural complexity. BYOL
also needs a momentum encoder with asymmetric networks. TS2Vec and TS-TCC,
while designed specifically for time series, have higher implementation
complexity.

SimCLR stands out for its simplicity: it requires only a shared encoder and
a projection head -- no momentum encoder, no memory bank. Despite this
simplicity, it has shown strong performance across domains.

Prior work has applied SimCLR successfully to bearing fault detection and
motor diagnosis. However, no previous work has applied SimCLR specifically
to automotive sensor fault detection -- this is the research gap we address.


SLIDE 6: PROPOSED FRAMEWORK [3:45 - 5:00]
--------------------------------------------------------------------------------
Our framework consists of three distinct phases.

In Phase 1, shown in green, we load real-world driving data from the Audi A2D2
dataset. This data undergoes preprocessing -- Z-score normalization and sliding
window segmentation with a window size of 200 samples and 50 percent overlap.
The windows are then augmented using three strategies: Gaussian jittering,
amplitude scaling, and temporal masking. The augmented views are used to train
a 1D-CNN encoder using the SimCLR contrastive learning objective. The key
outcome is a trained encoder that maps time-series windows into a
256-dimensional embedding space.

In Phase 2, shown in blue, we take a small amount of healthy HIL data -- just
90 seconds, yielding 89 windows -- and pass it through the frozen encoder.
From the resulting embeddings, we compute a healthy centroid and establish the
detection thresholds at six percentile levels.

In Phase 3, shown in orange, we process the fault-injected HIL data through
the same frozen encoder, compute the cosine similarity to the healthy centroid,
and compare against the thresholds.


SLIDE 7: DATASETS [5:00 - 6:00]
--------------------------------------------------------------------------------
We use two complementary data sources, which is a key aspect of this work.

The A2D2 dataset, from Audi, contains real-world driving recordings from Munich.
We used three recording sessions, yielding 219,064 samples -- approximately
36.5 minutes at 100 Hertz. We extract two sensor channels: the accelerator
pedal position in percent and the vehicle speed in kilometers per hour. The
speed signal is upsampled from 50 to 100 Hertz using linear interpolation.
Importantly, only healthy data is used for training.

The HIL dataset comes from a dSPACE Hardware-in-the-Loop test bench here at
TU Clausthal. It contains one healthy recording and six fault-injected
recordings covering three fault types applied to two sensors. The first 90
seconds of the healthy recording, yielding 89 windows, are used for
calibration.

Notice the cross-domain nature of this setup -- we train on real vehicle data
and test on simulation data.


SLIDE 8: FAULT TYPES [6:00 - 6:45]
--------------------------------------------------------------------------------
We evaluate three types of sensor faults, each modeling a different physical
failure mechanism.

The gain fault multiplies the true signal by a factor alpha, simulating
calibration errors or amplifier drift. The noise fault adds Gaussian noise,
simulating electromagnetic interference or loose connections. The stuck-at
fault freezes the sensor output at a constant value, simulating a complete
hardware failure.

The figure shows these three fault types applied to a sensor signal, with
the healthy baseline shown in the top left for comparison. Each fault is
expressed as a simple mathematical equation.


SLIDE 9: SimCLR AND ENCODER [6:45 - 7:45]
--------------------------------------------------------------------------------
The SimCLR framework operates as follows. Given an input window, we generate
two different augmented views. Both views pass through the same shared 1D-CNN
encoder, producing 256-dimensional embeddings. These are projected through a
two-layer projection head into 128-dimensional space where the NT-Xent
contrastive loss is computed.

The loss maximizes agreement between the two views of the same window -- the
positive pair -- while minimizing agreement with all other windows in the
batch -- 254 negative pairs. The temperature parameter is 0.5.

The encoder itself, shown at the bottom, is a three-block 1D-CNN. Block 1
uses 64 filters with kernel size 7, Block 2 uses 128 filters with kernel
size 5, and Block 3 uses 256 filters with kernel size 3. Adaptive average
pooling produces the final 256-dimensional embedding. The total model has
240,704 parameters -- a lightweight architecture enabling fast training.


SLIDE 10: AUGMENTATION AND PREPROCESSING [7:45 - 8:30]
--------------------------------------------------------------------------------
The data preprocessing pipeline consists of three steps.

First, Z-score normalization using the mean and standard deviation computed
from the A2D2 training data. Second, sliding window segmentation with a
window size of 200 samples and 50 percent overlap, producing 2,189 training
windows. Third, data augmentation.

The three augmentation strategies are shown in the figure: Gaussian jittering
with sigma of 0.1 simulates sensor noise, amplitude scaling between 0.8 and
1.2 simulates calibration variance, and temporal masking of 10 percent forces
temporal robustness. These ensure the encoder learns representations invariant
to minor perturbations while remaining sensitive to genuine fault patterns.


SLIDE 11: ANOMALY DETECTION METHOD [8:30 - 9:30]
--------------------------------------------------------------------------------
The anomaly detection process works as follows.

During calibration -- Phase 2 -- we pass 89 healthy HIL windows through the
frozen encoder, compute the mean embedding -- the healthy centroid -- and
calculate the cosine similarity of each healthy window to this centroid.
The thresholds are set at six percentile levels of this similarity
distribution: the 15th, 20th, 25th, 30th, 35th, and 40th.

During detection -- Phase 3 -- each test window is encoded, and its cosine
similarity to the healthy centroid is computed. If this similarity falls
below the threshold, the window is classified as faulty. If it is above,
the window is classified as healthy.

The intuition is straightforward: healthy data produces embeddings similar
to the healthy centroid, while faulty data produces embeddings that deviate
from it.


SLIDE 12: MULTI-THRESHOLD RESULTS [9:30 - 10:30]
--------------------------------------------------------------------------------
Now let me present the core results. The table on the right shows the
detection performance across all six thresholds.

The most striking finding is that per-fault precision is perfect -- 1.0 --
at every single threshold. This means that every window the system flags
as faulty within each fault scenario is indeed faulty. There are zero false
positives in the per-fault evaluation.

Recall increases monotonically from 64.6 percent at the 15th percentile to
92.9 percent at the 40th percentile. The F1-score correspondingly increases
from 0.780 to 0.963.

The plot on the left visualizes this trade-off. Since precision is already
perfect, increasing the threshold only improves recall without any precision
penalty. The optimal operating point is at the 40th percentile, where the
F1-score reaches its maximum of 0.963.

The ROC-AUC is 0.852, confirming that the learned representations provide
genuine discriminative power between healthy and faulty windows.


SLIDE 13: PER-FAULT RESULTS [10:30 - 11:30]
--------------------------------------------------------------------------------
The recall heatmap reveals important differences between fault types and
sensor locations.

Speed sensor faults are consistently easier to detect than accelerator faults.
At the 40th percentile, speed gain faults achieve 100 percent recall -- every
single window out of 321 is correctly detected. Speed noise reaches 95.6
percent and speed stuck-at reaches 91.7 percent.

Accelerator faults are harder to detect, ranging from 88.6 percent for
stuck-at to 90.8 percent for gain faults. This makes physical sense: the
accelerator signal has higher natural variability because drivers constantly
adjust the pedal, so some fault patterns overlap with normal driving behavior.
The speed signal, on the other hand, changes slowly due to vehicle inertia,
making any deviation more conspicuous.

This sensor-dependent and fault-dependent detectability reflects the physical
reality that more severe and unusual faults are inherently more
distinguishable from normal operation.


SLIDE 14: BINARY CLASSIFICATION [11:30 - 12:15]
--------------------------------------------------------------------------------
For the binary classification task -- simply distinguishing healthy from
faulty regardless of fault type -- the results at the 40th percentile show
an accuracy of 91.4 percent.

The confusion matrix shows 1,775 true positives and 53 true negatives,
with 36 false positives and 136 false negatives. The binary precision is
98.0 percent, recall is 92.9 percent, and F1-score is 0.954.

The confusion matrices on the left show how detection improves as we increase
the threshold percentile -- more true positives with a moderate increase in
false positives.


SLIDE 15: ROC AND SIMILARITY [12:15 - 13:00]
--------------------------------------------------------------------------------
The ROC curve on the left shows the trade-off between true positive rate
and false positive rate across all six thresholds. The area under the curve
is 0.852, well above the random baseline of 0.5.

The similarity distribution plot on the right provides visual insight into
why the system works: the healthy distribution is concentrated at high
similarity values near 1.0, while the faulty distribution is shifted toward
lower values. The vertical threshold lines show where the decision boundary
is placed at each percentile level.

The clear separation between these two distributions is what enables high
detection accuracy.


SLIDE 16: COMPUTING COSTS [13:00 - 13:45]
--------------------------------------------------------------------------------
An important practical consideration is the computational cost. The entire
end-to-end pipeline completes in under 54 seconds on CPU.

Training accounts for about half of this time at 26.6 seconds, but it is a
one-time cost -- the trained encoder is saved and reused. Inference is
extremely fast: embedding extraction for all fault windows takes about
1 second, corresponding to approximately 0.5 milliseconds per window. This
makes the system suitable for near-real-time deployment.

For comparison, the supervised CNN-GRU model by Ghannoum required 23,000
seconds -- over 6 hours -- of training on labeled data. Our self-supervised
approach is approximately 860 times faster while requiring no labeled data
at all.


SLIDE 17: CONCLUSION [13:45 - 14:45]
--------------------------------------------------------------------------------
To summarize the answers to our three research questions.

For RQ1: Self-supervised contrastive learning on healthy A2D2 data successfully
detects sensor faults in HIL data. The cross-domain transfer from real driving
to simulation works without any labeled fault examples, validating the core
hypothesis of this thesis.

For RQ2: Per-fault precision remains perfect at 1.0 across all six thresholds,
while recall ranges from 64.6 to 92.9 percent. The optimal balanced threshold
is at the 40th percentile with an F1-score of 0.963.

For RQ3: Speed gain faults are the most detectable at 100 percent recall,
while accelerator stuck-at faults are the hardest at 88.6 percent.
Detectability correlates with the physical severity of the fault and the
natural variability of the sensor signal.

The four key contributions of this work are: the first application of SimCLR
to automotive sensor fault detection, successful cross-domain transfer from
real driving data to HIL simulation, a comprehensive multi-threshold
evaluation with six fault scenarios, and a complete pipeline that runs in
under one minute on CPU.


SLIDE 18: FUTURE WORK [14:45 - 15:15]
--------------------------------------------------------------------------------
Several directions for future work emerge from this thesis.

In the short term, extending the framework to additional sensor channels such
as engine RPM, steering angle, and brake pressure would provide richer multi-
variate information. Implementing fault localization -- identifying which
specific sensor is faulty -- is another natural extension. Adaptive
thresholding would eliminate manual threshold selection.

In the longer term, comparing with alternative self-supervised methods such as
TS2Vec, MoCo, and BYOL would help identify the most effective approach.
Explainability techniques could provide diagnostic insights, and edge
deployment would enable real-time in-vehicle fault detection integrated with
the ISO 26262 safety framework.


SLIDE 19: THANK YOU [15:15 - 15:30]
--------------------------------------------------------------------------------
Thank you for your attention. I am happy to answer any questions.


================================================================================
POTENTIAL QUESTIONS AND PREPARED ANSWERS
================================================================================

Q1: Why is the loss reduction only 7%? Does the model converge properly?
A: The NT-Xent loss has a theoretical minimum that depends on the batch size
   (log(2N-1) for batch size N). With batch size 128, this minimum is
   approximately log(255) = 5.54 in natural log, or about 3.69 in the loss
   scale we use. Our final loss of 3.79 is very close to this theoretical
   minimum, indicating good convergence. The 7% reduction from 4.08 to 3.79
   represents meaningful learning despite appearing small in absolute terms.

Q2: Why is the ROC-AUC only 0.852 and not higher?
A: The ROC-AUC of 0.852 reflects the overlap between healthy and faulty
   similarity distributions. Some fault windows, particularly mild faults,
   produce embeddings similar to healthy data. However, the per-fault precision
   of 1.0 shows that when the system does flag a window, it is always correct.
   The AUC could be improved with more training data, a more powerful encoder,
   or additional sensor channels.

Q3: How does this compare to supervised approaches?
A: Our approach achieves 91.4% accuracy and 92.9% recall without any labeled
   data, compared to supervised methods that typically achieve 95-99% but
   require extensive labeled datasets. The key advantage is that our method
   works immediately on new fault types without retraining, while supervised
   methods fail on unseen fault categories.

Q4: Why only two sensors?
A: We focused on accelerator and speed as a proof of concept because they are
   the two sensors available in both A2D2 and our HIL setup. The framework is
   designed to scale to additional sensors -- the encoder's input channels can
   be increased, and the sliding window approach works with any number of
   channels.

Q5: Can this work in real-time in a vehicle?
A: Yes. Inference takes 0.5ms per window on CPU. A 2-second window at 100Hz
   gives us a 2-second detection latency, which is acceptable for most
   diagnostic applications. GPU deployment would reduce this further.

Q6: Why SimCLR and not other SSL methods like TS2Vec?
A: SimCLR was chosen for its simplicity, strong theoretical foundation, and
   proven effectiveness across domains. It requires no momentum encoder (unlike
   MoCo) and no asymmetric architecture (unlike BYOL). As shown in our
   comparison slide, SimCLR has the lowest architectural complexity while
   still providing strong contrastive learning. Future work should compare
   with time-series-specific methods like TS2Vec and TS-TCC.

Q7: How sensitive is the method to the calibration data size (90 seconds)?
A: 90 seconds yields 89 windows, which is sufficient to estimate a stable
   centroid in 256 dimensions. Preliminary experiments showed that even 30
   seconds provided reasonable results, though stability improves with more
   calibration data. This is a practical advantage -- only a brief healthy
   recording from the target system is needed.

Q8: Why is per-fault precision 1.0 but binary precision is 98%?
A: Per-fault precision is evaluated within each fault scenario separately --
   the system never misclassifies a healthy window when comparing only
   against faulty windows of the same type. Binary precision of 98% includes
   healthy windows being misclassified as faulty (36 false positives out of
   89 healthy windows), which occurs when healthy windows happen to fall
   below the threshold. This is expected with higher thresholds and represents
   a conservative over-detection behavior.

Q9: Why did you use cosine similarity instead of Euclidean distance?
A: Cosine similarity measures the angle between embeddings, making it invariant
   to the magnitude of the embedding vectors. This is important because
   contrastive learning optimizes for directional alignment, not absolute
   distance. Cosine similarity is also bounded between -1 and 1, providing
   a natural scale for threshold setting.
================================================================================
