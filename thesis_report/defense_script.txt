================================================================================
MASTER THESIS DEFENSE - SPEAKER SCRIPT
================================================================================
Total Duration: ~15 minutes (18 slides)
Timing guide per slide is marked with [TIME]
================================================================================


SLIDE 1: TITLE [0:00 - 0:30]
--------------------------------------------------------------------------------
Good morning/afternoon, esteemed committee members. My name is Yahia Amir Yahia
Gamal, and today I will present my master thesis on "Intelligent Analysis of
Automotive Sensor Faults Using Self-Supervised Learning and Real-Time
Simulation," supervised by Dr. Mohammad Abboush at the Institute of Software
and Systems Engineering.


SLIDE 2: OUTLINE [0:30 - 0:50]
--------------------------------------------------------------------------------
I have structured this presentation into ten parts. I will begin with the
motivation and problem statement, introduce the relevant background, then
present our three-phase framework, the datasets, the training process, and
finally the comprehensive evaluation results including computing costs. I
will conclude with a summary of findings and directions for future work.


SLIDE 3: MOTIVATION [0:50 - 2:00]
--------------------------------------------------------------------------------
Modern vehicles are equipped with over a hundred sensors that feed into
safety-critical systems such as adaptive cruise control, lane keeping, and
autonomous emergency braking. When these sensors develop faults -- for
instance, a miscalibrated accelerator pedal sensor or a noisy speed sensor --
the consequences can be severe, potentially leading to incorrect vehicle
behavior.

The current state of the art in sensor fault detection relies on supervised
learning, which requires large labeled datasets containing examples of every
possible fault type and severity. Obtaining such labeled data is expensive,
requires specialized test setups, and is inherently incomplete -- we cannot
anticipate every possible failure mode.

Our approach takes a fundamentally different path: we train exclusively on
healthy driving data using self-supervised contrastive learning, specifically
the SimCLR framework. The system learns what "normal" looks like, and then
detects faults as deviations from this learned normality. No labeled fault
examples are needed at any stage.

As shown in the V-model on the right, our work operates at the Hardware-in-
the-Loop testing level, which is a critical step in automotive validation
according to ISO 26262.


SLIDE 4: RESEARCH QUESTIONS [2:00 - 2:45]
--------------------------------------------------------------------------------
This thesis addresses three specific research questions.

First, can self-supervised contrastive learning on healthy driving data produce
representations that effectively detect sensor faults when applied to HIL
simulation data? This tests the core feasibility of the approach.

Second, how does the detection threshold affect the trade-off between precision
and recall? This is critical for practical deployment, where different safety
integrity levels demand different operating points.

Third, which fault types and sensor locations are most and least detectable, and
can we explain why from a physical perspective? Understanding this helps
identify the strengths and limitations of the approach.


SLIDE 5: PROPOSED FRAMEWORK [2:45 - 4:00]
--------------------------------------------------------------------------------
Our framework consists of three distinct phases.

In Phase 1, shown in green, we load real-world driving data from the Audi A2D2
dataset. This data undergoes preprocessing -- Z-score normalization and sliding
window segmentation with a window size of 200 samples and 50% overlap. The
windows are then augmented using three strategies: Gaussian jittering, amplitude
scaling, and temporal masking. The augmented views are used to train a 1D-CNN
encoder using the SimCLR contrastive learning objective. The key outcome is a
trained encoder that maps time-series windows into a 256-dimensional embedding
space.

In Phase 2, shown in blue, we take a small amount of healthy HIL data -- just
90 seconds -- and pass it through the frozen encoder. From the resulting
embeddings, we compute a healthy centroid and establish the similarity-based
detection thresholds at six percentile levels.

In Phase 3, shown in orange, we process the fault-injected HIL data through
the same frozen encoder, compute the cosine similarity of each window's
embedding to the healthy centroid, and compare it against the thresholds. If
the similarity falls below the threshold, the window is classified as faulty.


SLIDE 6: DATASETS [4:00 - 5:15]
--------------------------------------------------------------------------------
We use two complementary data sources, which is a key aspect of this work.

The A2D2 dataset, from Audi, contains real-world driving recordings from Munich.
We used three recording sessions, yielding 219,064 samples -- approximately
36 and a half minutes at 100 Hertz. We extract two sensor channels: the
accelerator pedal position in percent and the vehicle speed in kilometers per
hour. The speed signal is upsampled from 50 to 100 Hertz using linear
interpolation. Importantly, only healthy data is used for training.

The HIL dataset comes from a dSPACE Hardware-in-the-Loop test bench here at TU
Clausthal. It contains one healthy recording and six fault-injected recordings
covering three fault types applied to two sensors. The first 90 seconds of the
healthy recording are used for calibration.

The figure at the bottom illustrates this dataset fusion: the top panel shows
the A2D2 healthy speed signal, and the bottom shows the HIL speed signal with
the fault region highlighted. Notice the cross-domain nature of this setup --
we train on real vehicle data and test on simulation data.


SLIDE 7: FAULT TYPES [5:15 - 6:00]
--------------------------------------------------------------------------------
We evaluate three types of sensor faults, each modeling a different physical
failure mechanism.

The gain fault multiplies the true signal by a factor alpha, simulating
calibration errors or amplifier drift. The noise fault adds Gaussian noise,
simulating electromagnetic interference or loose connections. The stuck-at
fault freezes the sensor output at a constant value, simulating a complete
hardware failure.

The figure shows these three fault types applied to a real accelerator pedal
signal, with the healthy baseline shown in the top left for comparison.


SLIDE 8: SIMCLR ARCHITECTURE [6:00 - 7:00]
--------------------------------------------------------------------------------
The SimCLR framework operates as follows. Given an input window, we generate
two different augmented views. Both views pass through the same shared 1D-CNN
encoder, producing 256-dimensional embeddings. These embeddings are then
projected through a two-layer projection head into a 128-dimensional space
where the contrastive loss is computed.

The NT-Xent loss, shown at the bottom, maximizes the agreement between the two
views of the same window -- the positive pair -- while minimizing the agreement
with all other windows in the batch -- the negative pairs. With a batch size of
128, each sample has 254 negative pairs. The temperature parameter tau is set
to 0.5.

After training, the projection head is discarded, and only the encoder is kept
for anomaly detection.


SLIDE 9: ENCODER ARCHITECTURE [7:00 - 7:45]
--------------------------------------------------------------------------------
The encoder is a three-block 1D Convolutional Neural Network designed
specifically for time-series sensor data. The input is a two-channel window of
200 time steps.

Block 1 uses 64 filters with kernel size 7 and stride 2, followed by batch
normalization, ReLU, and max pooling. Block 2 increases to 128 filters with
kernel size 5. Block 3 further increases to 256 filters with kernel size 3.
Adaptive average pooling at the end collapses the temporal dimension into a
fixed-size 256-dimensional embedding vector.

The total model has approximately 241 thousand parameters -- a relatively
lightweight architecture that enables fast training and inference.


SLIDE 10: TRAINING RESULTS [7:45 - 8:30]
--------------------------------------------------------------------------------
The training curves show that the NT-Xent loss decreases rapidly in the first
ten epochs and then converges gradually. The initial loss of 4.08 decreases to
a final loss of 3.79, representing a 7 percent reduction.

Training completed in just 26.6 seconds on CPU, processing 2,189 windows over
50 epochs with 17 batches per epoch. This is remarkably fast compared to
supervised approaches that typically require hours of training on labeled data.

The three augmentation strategies -- jittering, scaling, and masking -- ensure
that the encoder learns representations that are invariant to minor
perturbations while remaining sensitive to genuine fault patterns.


SLIDE 11: ANOMALY DETECTION METHOD [8:30 - 9:30]
--------------------------------------------------------------------------------
The anomaly detection process works as follows.

During calibration, we pass 89 healthy HIL windows through the frozen encoder,
compute the mean embedding -- the healthy centroid -- and calculate the cosine
similarity of each healthy window to this centroid. The thresholds are set at
six percentile levels of this similarity distribution.

During detection, each test window is encoded, and its cosine similarity to
the healthy centroid is computed. If this similarity falls below the threshold,
the window is classified as faulty. The intuition is straightforward: healthy
data produces embeddings similar to the healthy centroid, while faulty data
produces embeddings that deviate from it.


SLIDE 12: MULTI-THRESHOLD RESULTS [9:30 - 10:45]
--------------------------------------------------------------------------------
Now let me present the core results. The table on the right shows the detection
performance across all six thresholds.

The most striking finding is that precision is perfect -- 1.0 -- at every single
threshold. This means that every window the system flags as faulty is indeed
faulty. There are zero false positives in the per-fault evaluation.

Recall increases monotonically from 64.6 percent at the 15th percentile to
92.9 percent at the 40th percentile. The F1-score correspondingly increases
from 0.78 to 0.963.

The plot on the left visualizes this trade-off. Since precision is already
perfect, increasing the threshold only improves recall without any precision
penalty. The optimal operating point is at the 40th percentile, where the
F1-score reaches its maximum of 0.963.

The ROC-AUC is 0.852, confirming that the learned representations provide
genuine discriminative power between healthy and faulty windows.


SLIDE 13: PER-FAULT RESULTS [10:45 - 12:00]
--------------------------------------------------------------------------------
The recall heatmap reveals important differences between fault types and sensor
locations.

Speed sensor faults are consistently easier to detect than accelerator faults.
At the 40th percentile, speed gain faults achieve 100 percent recall -- every
single faulty window is detected. Speed noise reaches 95.6 percent and speed
stuck-at reaches 91.7 percent.

Accelerator faults are harder to detect, ranging from 88.6 percent for stuck-at
to 90.8 percent for gain faults. This makes physical sense: the accelerator
signal has higher natural variability because drivers constantly adjust the
pedal, so some fault patterns overlap with normal driving behavior. The speed
signal, on the other hand, changes slowly due to vehicle inertia, making any
deviation more conspicuous.

This sensor-dependent and fault-dependent detectability is actually a strength
of the approach -- it reflects the physical reality that more severe and unusual
faults are inherently more distinguishable from normal operation.


SLIDE 14: BINARY CLASSIFICATION AND ROC [12:00 - 13:00]
--------------------------------------------------------------------------------
For the binary classification task -- simply distinguishing healthy from faulty
regardless of fault type -- the results at the 40th percentile show an accuracy
of 91.4 percent, with 1,775 true positives and only 36 false positives out of
89 healthy windows.

The confusion matrices on the left show how the detection improves as we
increase the threshold percentile -- more true positives with a moderate
increase in false positives.

The similarity distribution plot on the right provides visual insight into why
the system works: the healthy distribution is concentrated at high similarity
values near 1.0, while the faulty distribution is shifted toward lower values.
The vertical threshold lines show where the decision boundary is placed at
each percentile level.


SLIDE 15: COMPUTING COSTS [13:00 - 13:45]
--------------------------------------------------------------------------------
An important practical consideration is the computational cost. The entire
end-to-end pipeline completes in under 54 seconds on CPU.

Training accounts for about half of this time at 26.6 seconds, but it is a
one-time cost -- the trained encoder is saved and reused. Inference is
extremely fast: embedding extraction for all 2,000 fault windows takes just
1 second, corresponding to approximately 0.5 milliseconds per window. This
makes the system suitable for near-real-time deployment.

For comparison, the supervised CNN-GRU model by Ghannoum required 23,000
seconds -- over 6 hours -- of training on labeled data. Our self-supervised
approach is approximately 860 times faster while requiring no labeled data
at all.


SLIDE 16: CONCLUSION [13:45 - 14:45]
--------------------------------------------------------------------------------
To summarize the answers to our three research questions.

For RQ1: Self-supervised contrastive learning on healthy A2D2 data successfully
detects sensor faults in HIL data. The cross-domain transfer from real driving
to simulation works without any labeled fault examples, validating the core
hypothesis of this thesis.

For RQ2: Precision remains perfect at 1.0 across all six thresholds, while
recall ranges from 64.6 to 92.9 percent. The optimal balanced threshold is at
the 40th percentile with an F1-score of 0.963.

For RQ3: Speed gain faults are the most detectable at 100 percent recall, while
accelerator stuck-at faults are the hardest at 88.6 percent. Detectability
correlates with the physical severity of the fault and the natural variability
of the sensor signal.

The four key contributions of this work are: the first application of SimCLR to
automotive sensor fault detection, successful cross-domain transfer from real
driving data to HIL simulation, a comprehensive multi-threshold evaluation with
six fault scenarios, and a complete pipeline that runs in under one minute.


SLIDE 17: FUTURE WORK [14:45 - 15:15]
--------------------------------------------------------------------------------
Several directions for future work emerge from this thesis.

In the short term, extending the framework to additional sensor channels such
as engine RPM, steering angle, and brake pressure would provide richer multi-
variate information. Implementing fault localization -- identifying which
specific sensor is faulty -- is another natural extension. Adaptive thresholding
mechanisms could eliminate the need for manual threshold selection.

In the longer term, comparing with alternative self-supervised methods such as
TS2Vec, MoCo, and BYOL would help identify the most effective approach.
Explainability techniques could provide diagnostic insights, and edge deployment
would enable real-time in-vehicle fault detection integrated with the ISO 26262
safety framework.


SLIDE 18: THANK YOU [15:15 - 15:30]
--------------------------------------------------------------------------------
Thank you for your attention. I am happy to answer any questions.


================================================================================
POTENTIAL QUESTIONS AND PREPARED ANSWERS
================================================================================

Q1: Why is the loss reduction only 7%? Does the model converge properly?
A: The NT-Xent loss has a theoretical minimum that depends on the batch size
   (log(2N-1) for batch size N). With batch size 128, this minimum is
   approximately log(255) = 5.54 in natural log, or about 3.69 in the loss
   scale we use. Our final loss of 3.79 is very close to this theoretical
   minimum, indicating good convergence. The 7% reduction from 4.08 to 3.79
   represents meaningful learning despite appearing small in absolute terms.

Q2: Why is the ROC-AUC only 0.852 and not higher?
A: The ROC-AUC of 0.852 reflects the overlap between healthy and faulty
   similarity distributions. Some fault windows, particularly mild faults,
   produce embeddings similar to healthy data. However, the per-fault precision
   of 1.0 shows that when the system does flag a window, it is always correct.
   The AUC could be improved with more training data, a more powerful encoder,
   or additional sensor channels.

Q3: How does this compare to supervised approaches?
A: Our approach achieves 91.4% accuracy and 92.9% recall without any labeled
   data, compared to supervised methods that typically achieve 95-99% but
   require extensive labeled datasets. The key advantage is that our method
   works immediately on new fault types without retraining, while supervised
   methods fail on unseen fault categories.

Q4: Why only two sensors?
A: We focused on accelerator and speed as a proof of concept because they are
   the two sensors available in both A2D2 and our HIL setup. The framework is
   designed to scale to additional sensors -- the encoder's input channels can
   be increased, and the sliding window approach works with any number of
   channels.

Q5: Can this work in real-time in a vehicle?
A: Yes. Inference takes 0.5ms per window on CPU. A 2-second window at 100Hz
   gives us a 2-second detection latency, which is acceptable for most
   diagnostic applications. GPU deployment would reduce this further.

Q6: Why SimCLR and not other SSL methods like TS2Vec?
A: SimCLR was chosen for its simplicity, strong theoretical foundation, and
   proven effectiveness across domains. It requires no momentum encoder (unlike
   MoCo) and no asymmetric architecture (unlike BYOL). Future work should
   compare with time-series-specific methods like TS2Vec and TS-TCC.

Q7: How sensitive is the method to the calibration data size (90 seconds)?
A: 90 seconds yields 89 windows, which is sufficient to estimate a stable
   centroid in 256 dimensions. Preliminary experiments showed that even 30
   seconds provided reasonable results, though stability improves with more
   calibration data. This is a practical advantage -- only a brief healthy
   recording from the target system is needed.
================================================================================
