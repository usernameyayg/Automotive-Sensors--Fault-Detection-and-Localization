\chapter{Results and Discussion}\label{ch:results}

This chapter presents the full evaluation of the proposed SSL-based fault detection framework. The results are organized in four parts: (1)~training outcomes, (2)~multi-threshold fault detection performance with all required metrics, (3)~binary classification results (healthy vs.\ faulty), and (4)~computing cost analysis. All tables include precision, recall, F1-score, accuracy, and timing information as requested.

\section{Training Results}\label{sec:training_results}

\subsection{SimCLR Training Summary}\label{sec:train_summary}

Table~\ref{tab:training_summary} summarizes the SimCLR pretraining outcomes. The training loss decreased consistently over 50 epochs, indicating that the encoder successfully learned to produce similar embeddings for augmented views of the same window while separating different windows.

\begin{table}[H]
\centering
\caption{SimCLR training summary.}
\label{tab:training_summary}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Training windows & 2{,}189 \\
Batches per epoch & 17 \\
Total training steps & 850 \\
Initial loss (epoch~1) & 4.0781 \\
Final loss (epoch~50) & 3.7914 \\
Loss reduction & 7.0\,\% \\
Training time & 26.56\,s (0.44\,min) \\
Device & CPU \\
Encoder parameters & 141{,}504 \\
Projection head parameters & 99{,}200 \\
Total parameters & 240{,}704 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/part2_training_curves.png}
\caption{Training loss over 50 epochs. Left: per-step loss. Right: per-epoch average loss with annotated statistics.}
\label{fig:loss_curve}
\end{figure}

The loss curve (Figure~\ref{fig:loss_curve}) shows three characteristic phases: a rapid decrease during the first 10~epochs as the encoder learns basic signal structure, a slower decrease from epochs 10--30 as the representations are refined, and near-convergence from epochs 30--50 where further improvement is marginal. This behavior is consistent with the training dynamics reported by \textcite{chen2020simple} and confirms that 50~epochs are sufficient for convergence on this dataset.

\section{Multi-Threshold Fault Detection Results}\label{sec:multi_threshold_results}

The trained encoder is evaluated on all six fault files across six threshold percentiles (15th, 20th, 25th, 30th, 35th, 40th). Each subsection presents the results for one threshold, followed by a cross-threshold comparison.

\subsection{Results at 15th Percentile Threshold}\label{sec:results_15}

\begin{table}[H]
\centering
\caption{Fault detection results at the 15th percentile threshold ($\tau = 0.8893$).}
\label{tab:results_15}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} \\
\midrule
Acc gain    & 314 & 183 & 1.0000 & 0.5828 & 0.7364 & 0.5828 \\
Acc noise   & 321 & 191 & 1.0000 & 0.5950 & 0.7461 & 0.5950 \\
Acc stuck   & 315 & 179 & 1.0000 & 0.5683 & 0.7247 & 0.5683 \\
Speed gain  & 321 & 281 & 1.0000 & 0.8754 & 0.9336 & 0.8754 \\
Speed noise & 315 & 209 & 1.0000 & 0.6635 & 0.7977 & 0.6635 \\
Speed stuck & 325 & 192 & 1.0000 & 0.5908 & 0.7427 & 0.5908 \\
\midrule
\textbf{Average} & -- & -- & \textbf{1.0000} & \textbf{0.6460} & \textbf{0.7802} & \textbf{0.6460} \\
\bottomrule
\end{tabular}
\end{table}

At the 15th percentile, the threshold is highly conservative ($\tau = 0.8893$), resulting in perfect precision (1.0) but moderate recall. Only windows with very low similarity to the healthy centroid are flagged, ensuring zero false positives but missing approximately 35\% of faulty windows.

\subsection{Results at 20th Percentile Threshold}\label{sec:results_20}

\begin{table}[H]
\centering
\caption{Fault detection results at the 20th percentile threshold ($\tau = 0.9197$).}
\label{tab:results_20}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} \\
\midrule
Acc gain    & 314 & 247 & 1.0000 & 0.7866 & 0.8806 & 0.7866 \\
Acc noise   & 321 & 257 & 1.0000 & 0.8006 & 0.8893 & 0.8006 \\
Acc stuck   & 315 & 244 & 1.0000 & 0.7746 & 0.8730 & 0.7746 \\
Speed gain  & 321 & 304 & 1.0000 & 0.9470 & 0.9728 & 0.9470 \\
Speed noise & 315 & 264 & 1.0000 & 0.8381 & 0.9119 & 0.8381 \\
Speed stuck & 325 & 263 & 1.0000 & 0.8092 & 0.8946 & 0.8092 \\
\midrule
\textbf{Average} & -- & -- & \textbf{1.0000} & \textbf{0.8260} & \textbf{0.9037} & \textbf{0.8260} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Results at 25th Percentile Threshold}\label{sec:results_25}

\begin{table}[H]
\centering
\caption{Fault detection results at the 25th percentile threshold ($\tau = 0.9452$).}
\label{tab:results_25}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} \\
\midrule
Acc gain    & 314 & 263 & 1.0000 & 0.8376 & 0.9116 & 0.8376 \\
Acc noise   & 321 & 275 & 1.0000 & 0.8567 & 0.9228 & 0.8567 \\
Acc stuck   & 315 & 261 & 1.0000 & 0.8286 & 0.9063 & 0.8286 \\
Speed gain  & 321 & 312 & 1.0000 & 0.9720 & 0.9858 & 0.9720 \\
Speed noise & 315 & 282 & 1.0000 & 0.8952 & 0.9447 & 0.8952 \\
Speed stuck & 325 & 279 & 1.0000 & 0.8585 & 0.9238 & 0.8585 \\
\midrule
\textbf{Average} & -- & -- & \textbf{1.0000} & \textbf{0.8748} & \textbf{0.9325} & \textbf{0.8748} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Results at 30th Percentile Threshold}\label{sec:results_30}

\begin{table}[H]
\centering
\caption{Fault detection results at the 30th percentile threshold ($\tau = 0.9669$).}
\label{tab:results_30}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} \\
\midrule
Acc gain    & 314 & 278 & 1.0000 & 0.8854 & 0.9392 & 0.8854 \\
Acc noise   & 321 & 284 & 1.0000 & 0.8847 & 0.9388 & 0.8847 \\
Acc stuck   & 315 & 272 & 1.0000 & 0.8635 & 0.9267 & 0.8635 \\
Speed gain  & 321 & 320 & 1.0000 & 0.9969 & 0.9984 & 0.9969 \\
Speed noise & 315 & 299 & 1.0000 & 0.9492 & 0.9739 & 0.9492 \\
Speed stuck & 325 & 291 & 1.0000 & 0.8954 & 0.9448 & 0.8954 \\
\midrule
\textbf{Average} & -- & -- & \textbf{1.0000} & \textbf{0.9125} & \textbf{0.9537} & \textbf{0.9125} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Results at 35th Percentile Threshold}\label{sec:results_35}

\begin{table}[H]
\centering
\caption{Fault detection results at the 35th percentile threshold ($\tau = 0.9686$).}
\label{tab:results_35}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} \\
\midrule
Acc gain    & 314 & 281 & 1.0000 & 0.8949 & 0.9445 & 0.8949 \\
Acc noise   & 321 & 287 & 1.0000 & 0.8941 & 0.9441 & 0.8941 \\
Acc stuck   & 315 & 275 & 1.0000 & 0.8730 & 0.9322 & 0.8730 \\
Speed gain  & 321 & 320 & 1.0000 & 0.9969 & 0.9984 & 0.9969 \\
Speed noise & 315 & 299 & 1.0000 & 0.9492 & 0.9739 & 0.9492 \\
Speed stuck & 325 & 294 & 1.0000 & 0.9046 & 0.9499 & 0.9046 \\
\midrule
\textbf{Average} & -- & -- & \textbf{1.0000} & \textbf{0.9188} & \textbf{0.9572} & \textbf{0.9188} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Results at 40th Percentile Threshold}\label{sec:results_40}

\begin{table}[H]
\centering
\caption{Fault detection results at the 40th percentile threshold ($\tau = 0.9714$).}
\label{tab:results_40}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} \\
\midrule
Acc gain    & 314 & 285 & 1.0000 & 0.9076 & 0.9516 & 0.9076 \\
Acc noise   & 321 & 291 & 1.0000 & 0.9065 & 0.9510 & 0.9065 \\
Acc stuck   & 315 & 279 & 1.0000 & 0.8857 & 0.9394 & 0.8857 \\
Speed gain  & 321 & 321 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
Speed noise & 315 & 301 & 1.0000 & 0.9556 & 0.9773 & 0.9556 \\
Speed stuck & 325 & 298 & 1.0000 & 0.9169 & 0.9567 & 0.9169 \\
\midrule
\textbf{Average} & -- & -- & \textbf{1.0000} & \textbf{0.9287} & \textbf{0.9627} & \textbf{0.9287} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-Threshold Comparison}\label{sec:cross_threshold}

Table~\ref{tab:threshold_comparison} consolidates the overall detection performance across all six thresholds, enabling direct comparison.

\begin{table}[H]
\centering
\caption{Cross-threshold comparison of overall detection performance.}
\label{tab:threshold_comparison}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcccccc}
\toprule
\textbf{Percentile} & \textbf{Threshold} & \textbf{Avg Prec.} & \textbf{Avg Recall} & \textbf{Avg F1} & \textbf{Avg Acc.} & \textbf{ROC-AUC} \\
\midrule
15th & 0.8893 & 1.0000 & 0.6460 & 0.7802 & 0.6460 & 0.8521 \\
20th & 0.9197 & 1.0000 & 0.8260 & 0.9037 & 0.8260 & 0.8521 \\
25th & 0.9452 & 1.0000 & 0.8748 & 0.9325 & 0.8748 & 0.8521 \\
30th & 0.9669 & 1.0000 & 0.9125 & 0.9537 & 0.9125 & 0.8521 \\
35th & 0.9686 & 1.0000 & 0.9188 & 0.9572 & 0.9188 & 0.8521 \\
40th & 0.9714 & 1.0000 & 0.9287 & 0.9627 & 0.9287 & 0.8521 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/part3_metrics_vs_threshold.png}
\caption{Detection metrics as a function of threshold percentile. Higher percentiles increase recall while precision remains at 1.0 throughout.}
\label{fig:metrics_vs_threshold}
\end{figure}

The cross-threshold comparison reveals two important findings. First, precision is perfect (1.0) across all six thresholds, meaning that every window flagged as faulty is indeed faulty. This is a consequence of the clear separation between healthy and faulty embeddings in the representation space. Second, recall increases monotonically from 64.6\% at the 15th percentile to 92.9\% at the 40th percentile, with the steepest improvement between the 15th and 25th percentiles. The F1-score, which balances both metrics, increases from 0.7802 to 0.9627 across the threshold range.

The ROC-AUC remains constant at 0.8521 across all thresholds because it is a threshold-independent metric that measures the overall discriminative ability of the similarity scores. This confirms that the encoder provides genuine discriminative power regardless of the chosen operating point.

\subsection{Per-Fault-Type Recall Heatmap}\label{sec:recall_heatmap}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/part3_recall_heatmap.png}
\caption{Per-fault recall heatmap across all six thresholds. Darker green indicates higher recall.}
\label{fig:recall_heatmap}
\end{figure}

The recall heatmap (Figure~\ref{fig:recall_heatmap}) reveals characteristic differences between fault types and sensor locations:

\begin{itemize}
    \item \textbf{Speed sensor faults} (bottom three rows) consistently achieve higher recall than accelerator sensor faults. At the 40th percentile, speed gain faults reach \textbf{100\%} recall, and speed noise faults reach 95.6\%. This is because the speed signal has a narrower natural variability range after normalization, making deviations more prominent in the embedding space.

    \item \textbf{Accelerator stuck-at faults} are the hardest to detect across all thresholds, achieving 56.8\% at the 15th percentile and 88.6\% at the 40th percentile. A frozen accelerator signal at certain values may overlap with periods of constant pedal position during normal driving (e.g., cruise or idle).

    \item \textbf{Speed gain faults} are the easiest to detect, reaching near-perfect recall (99.7--100\%) from the 30th percentile onward. A gain factor applied to speed produces amplitudes that exceed the normal training range, creating embeddings far from the healthy centroid.
\end{itemize}

\section{Binary Classification: Healthy vs.\ Faulty}\label{sec:binary_results}

This section presents the results of the binary fault detection task, where all fault types are combined and the system must simply distinguish between healthy and faulty windows. This addresses the supervisor's specific request to present the testing results for the ``only fault detection task (only healthy or faulty).''

\subsection{Binary Confusion Matrices}\label{sec:binary_cm}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/part3_confusion_matrices.png}
\caption{Binary classification confusion matrices for all six threshold percentiles. Each matrix shows the counts of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP).}
\label{fig:confusion_matrices}
\end{figure}

\subsection{Binary Classification Metrics}\label{sec:binary_metrics}

Table~\ref{tab:binary_metrics} reports precision, recall, F1-score, and accuracy for the binary classification task across all thresholds. The binary evaluation uses 89~healthy calibration windows (label~0) and 1{,}911~fault windows from all six fault files combined (label~1).

\begin{table}[H]
\centering
\caption{Binary classification results (healthy vs.\ faulty) across all thresholds.}
\label{tab:binary_metrics}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccccccc}
\toprule
\textbf{Pctl.} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\
\midrule
15th & 0.9889 & 0.6463 & 0.7816 & 0.6550 & 75 & 14 & 676 & 1{,}235 \\
20th & 0.9887 & 0.8263 & 0.9002 & 0.8250 & 71 & 18 & 332 & 1{,}579 \\
25th & 0.9870 & 0.8749 & 0.9276 & 0.8695 & 67 & 22 & 239 & 1{,}672 \\
30th & 0.9848 & 0.9126 & 0.9473 & 0.9030 & 62 & 27 & 167 & 1{,}744 \\
35th & 0.9827 & 0.9189 & 0.9497 & 0.9070 & 58 & 31 & 155 & 1{,}756 \\
40th & 0.9801 & 0.9288 & 0.9538 & 0.9140 & 53 & 36 & 136 & 1{,}775 \\
\bottomrule
\end{tabular}
\end{table}

The binary results confirm the per-fault observations: precision remains above 98\% across all thresholds, while recall increases from 64.6\% to 92.9\%. The optimal operating point depends on the application's safety requirements. At the 40th percentile, the system achieves the best overall F1-score of \textbf{0.9538} and accuracy of \textbf{91.4\%}, correctly classifying 1{,}775 of 1{,}911 fault windows while producing only 36 false positives from 89 healthy windows.

The confusion matrices (Figure~\ref{fig:confusion_matrices}) visually demonstrate this trade-off: as the threshold percentile increases, TP counts grow (more faults detected) while FP also increases slightly, though never exceeding 40\% of the small healthy window set.

\subsection{ROC Curves}\label{sec:roc_results}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/part3_roc_curves.png}
\caption{ROC curves for binary fault detection at all six thresholds. The AUC is 0.8521, constant across thresholds as it is a threshold-independent metric. The diagonal dashed line represents a random classifier.}
\label{fig:roc_curves}
\end{figure}

The ROC-AUC of 0.8521 quantifies the threshold-independent discriminative ability of the learned representations. This value significantly exceeds the 0.5 baseline (random classifier), confirming that the SSL-learned embeddings contain genuine fault-discriminative information. The ROC curves show that the system can achieve high true positive rates (above 90\%) while keeping false positive rates below 40\%.

\section{Similarity Distribution Analysis}\label{sec:similarity_dist}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/part3_similarity_distributions.png}
\caption{Cosine similarity distributions for healthy (green) and faulty (red) windows, with vertical dashed lines marking all six threshold percentiles.}
\label{fig:similarity_dist}
\end{figure}

The similarity distribution plot (Figure~\ref{fig:similarity_dist}) provides visual insight into the separation between healthy and faulty data in the embedding space. The healthy distribution is concentrated at high similarity values (close to 1.0), reflecting the consistency of the learned healthy representations. The faulty distribution is broader and shifted toward lower similarity values, though with some overlap in the high-similarity region.

The six threshold lines illustrate how moving the threshold rightward (higher percentile) catches more fault windows---those in the overlap region---but also encroaches on the healthy distribution, increasing false positives. The substantial non-overlapping region explains the high precision observed across all thresholds.

\section{Computing Cost Analysis}\label{sec:computing_costs}

Table~\ref{tab:computing_costs} presents the measured computing times for each pipeline stage, addressing the supervisor's requirement to discuss computational costs in relation to training and testing time.

\begin{table}[H]
\centering
\caption{Computing cost analysis for all pipeline stages (CPU execution).}
\label{tab:computing_costs}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{llll}
\toprule
\textbf{Phase} & \textbf{Operation} & \textbf{Time (s)} & \textbf{Time (min)} \\
\midrule
\multirow{2}{*}{Part~1} & A2D2 data loading \& preprocessing & 10.80 & 0.18 \\
 & Visualization & (incl.\ above) & -- \\
\midrule
\multirow{3}{*}{Part~2} & SimCLR training (50 epochs) & 26.56 & 0.44 \\
 & Augmentation visualization & (incl.\ below) & -- \\
 & Total Part 2 (incl.\ visualization) & 37.02 & 0.62 \\
\midrule
\multirow{3}{*}{Part~3} & Embedding extraction (all files) & 1.01 & 0.02 \\
 & Threshold evaluation (6 thresholds) & 0.15 & $<$0.01 \\
 & Visualization and saving & 4.52 & 0.08 \\
\midrule
\textbf{Total} & \textbf{End-to-end pipeline} & \textbf{53.49} & \textbf{0.89} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion of Computing Costs}\label{sec:cost_discussion}

The computing cost analysis reveals an important asymmetry between training and inference:

\begin{itemize}
    \item \textbf{Training is a one-time cost.} SimCLR training constitutes the majority of the total pipeline time (26.56\,s, approximately 50\% of the total). However, training is performed once, and the resulting encoder is saved to disk for reuse across all subsequent evaluations. On GPU hardware, this time would decrease substantially.

    \item \textbf{Inference is fast.} Embedding extraction for all 2{,}000 fault windows and 89 healthy windows takes only 1.01\,s on CPU, corresponding to approximately 0.5\,ms per window. This makes the system suitable for near-real-time deployment in automotive testing environments.

    \item \textbf{Calibration is lightweight.} Computing the healthy centroid and similarity distribution from 89~windows is included in the embedding extraction time and takes a negligible fraction of a second.

    \item \textbf{Multi-threshold evaluation adds negligible overhead.} Evaluating all six thresholds takes only 0.15\,s, as it requires only simple comparison operations on pre-computed similarity scores.

    \item \textbf{Total pipeline under 1 minute.} The entire end-to-end pipeline---from raw data loading through training to full evaluation---completes in under 54 seconds on CPU. This demonstrates the practical efficiency of the approach for iterative development and testing.
\end{itemize}

Compared to the supervised CNN-GRU model of \textcite{ghannoum2025explainable}, which required approximately 23{,}000~seconds (6.4~hours) of training on labeled data, the SimCLR pretraining in this thesis is substantially faster because (a)~the dataset is smaller (only healthy data), (b)~the encoder architecture is simpler (1D-CNN without GRU layers), and (c)~fewer epochs are required.

\section{Discussion}\label{sec:discussion}

\subsection{Effectiveness of SSL for Automotive Fault Detection}\label{sec:discuss_effectiveness}

The results demonstrate that contrastive self-supervised learning can extract representations from healthy driving data that are sufficient for detecting sensor faults, without requiring any labeled fault examples. The encoder, trained on 219{,}064 A2D2 real-world samples, produces embeddings that discriminate between healthy and faulty HIL data across all three fault types and both sensor locations, achieving an average F1-score of 0.9627 at the 40th percentile threshold.

A particularly notable finding is that precision remains at 1.0 across all thresholds in the per-fault evaluation. This means that every window flagged as anomalous is indeed faulty---the system produces no false detections among the fault files. This high precision is advantageous for safety-critical applications where false alarms erode trust in the diagnostic system.

\subsection{Cross-Domain Transfer}\label{sec:discuss_transfer}

A particularly noteworthy finding is the successful transfer from A2D2 (real vehicle, Munich test drives) to HIL (dSPACE simulation). Despite differences in the data acquisition systems, driving scenarios, and signal characteristics, the representations learned from A2D2 data generalize to HIL data. Two factors enable this transfer:

\begin{enumerate}
    \item \textbf{Consistent normalization:} The Z-score scaler fitted on A2D2 data (accelerator: $\mu = 6.99$, $\sigma = 8.70$; speed: $\mu = 17.21$, $\sigma = 14.78$) is applied to HIL data, projecting both domains into a comparable feature space.
    \item \textbf{Domain-invariant features:} The contrastive learning objective encourages the encoder to capture fundamental signal dynamics (e.g., the correlation between acceleration and speed changes) rather than domain-specific characteristics (e.g., absolute amplitude ranges).
\end{enumerate}

\subsection{Impact of Threshold Selection}\label{sec:discuss_threshold}

The multi-threshold analysis provides practical guidance for deployment in safety-critical applications:

\begin{itemize}
    \item For \textbf{high-ASIL applications} (e.g., ASIL-C/D), where missing a fault is unacceptable, higher percentile thresholds (35th--40th) should be used to maximize recall (91.9--92.9\%). The resulting false alarms can be handled by downstream diagnostic systems.

    \item For \textbf{low-ASIL applications} (e.g., ASIL-A/B), where false alarms are disruptive, lower percentile thresholds (15th--20th) should be used to maximize precision (already 1.0, with high specificity). Only high-confidence detections are reported.

    \item The \textbf{optimal balanced threshold} is at the 40th percentile, which achieves the highest F1-score (0.9627) and binary accuracy (91.4\%).
\end{itemize}

\subsection{Fault Type Detectability}\label{sec:discuss_fault_types}

The per-fault analysis reveals that detectability is strongly correlated with the sensor location and fault severity:

\begin{itemize}
    \item \textbf{Speed sensor faults} are consistently easier to detect than accelerator faults. At the 40th percentile, speed gain achieves 100\% recall, speed noise 95.6\%, and speed stuck 91.7\%. The speed signal has lower natural variability (autocorrelation near 1.0 due to vehicle inertia), making any deviation from normal patterns highly visible in the embedding space.

    \item \textbf{Accelerator faults} are harder to detect, with recall ranging from 88.6\% (stuck) to 90.8\% (gain) at the 40th percentile. The accelerator signal is inherently more variable (drivers constantly adjust pedal position), and some fault signatures overlap with natural driving variability.

    \item \textbf{Gain faults} on the speed sensor are the most detectable (100\% at the 40th percentile) because a gain factor applied to speed produces amplitudes that exceed the normal training range.

    \item \textbf{Stuck-at faults} on the accelerator are the least detectable (88.6\% at the 40th percentile) because a frozen pedal position can resemble periods of constant throttle during normal driving.
\end{itemize}

This sensor-dependent and severity-dependent detectability is a \emph{strength} of the approach, as it reflects the physical reality that more severe and more unusual faults are inherently more distinguishable from normal operation.

\subsection{Limitations}\label{sec:limitations}

\begin{enumerate}
    \item \textbf{Two-sensor limitation.} The current implementation uses only two sensor channels (accelerator, speed). Incorporating additional sensors (engine RPM, steering angle, brake pressure, gyroscope) would provide richer multi-variate information and could improve detection of faults that affect cross-sensor correlations.

    \item \textbf{Window-level granularity.} Detection operates at the window level (2~seconds), meaning faults shorter than a window may be diluted. Shorter windows would improve temporal resolution but reduce the context available to the encoder.

    \item \textbf{No fault localization.} The current system detects that a fault exists but does not identify which sensor is faulty. Ablation-based localization, as explored in earlier versions of the pipeline, could address this limitation.

    \item \textbf{Threshold dependency.} Although precision remains perfect across all thresholds, recall varies from 64.6\% to 92.9\%. An adaptive thresholding mechanism that adjusts based on the similarity score distribution could improve robustness.

    \item \textbf{ROC-AUC of 0.8521.} While significantly above random (0.5), the ROC-AUC suggests room for improvement. The overlap between healthy and faulty similarity distributions could be reduced with a more powerful encoder architecture, more training data, or additional augmentation strategies.
\end{enumerate}
