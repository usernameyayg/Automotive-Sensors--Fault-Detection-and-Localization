\chapter{Results and Discussion}\label{ch:results}

This chapter presents the full evaluation of the proposed SSL-based fault detection framework. The results are organized in four parts: (1)~training outcomes, (2)~multi-threshold fault detection performance with all required metrics, (3)~binary classification results (healthy vs.\ faulty), and (4)~computing cost analysis. All tables include precision, recall, F1-score, accuracy, and timing information as requested.

\section{Training Results}\label{sec:training_results}

\subsection{SimCLR Training Summary}\label{sec:train_summary}

Table~\ref{tab:training_summary} summarizes the SimCLR pretraining outcomes. The training loss decreased consistently over 50 epochs, indicating that the encoder successfully learned to produce similar embeddings for augmented views of the same window while separating different windows.

\begin{table}[H]
\centering
\caption{SimCLR training summary.}
\label{tab:training_summary}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Training windows & $\sim$2,189 \\
Batches per epoch & $\sim$17 \\
Total training steps & $\sim$850 \\
Initial loss (epoch~1) & \textit{(from notebook output)} \\
Final loss (epoch~50) & \textit{(from notebook output)} \\
Training time & \textit{(from notebook output, see Table~\ref{tab:computing_costs})} \\
Device & CPU / GPU \textit{(specify from your run)} \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill in the exact values from your notebook execution.}
\end{table}

\begin{figure}[H]
\centering
\fbox{\parbox{0.92\textwidth}{\centering\vspace{4cm}
\textbf{[PLACEHOLDER -- Figure~\ref{fig:loss_curve}]}\\[0.5em]
Use: \texttt{part2\_training\_curves.png} from the notebook output.\\
Shows training loss (y-axis) vs.\ training step and epoch (x-axis).
\vspace{0.5cm}}}
\caption{Training loss over 50 epochs. Left: per-step loss. Right: per-epoch average loss.}
\label{fig:loss_curve}
\end{figure}

The loss curve (Figure~\ref{fig:loss_curve}) shows three characteristic phases: a rapid decrease during the first 10~epochs as the encoder learns basic signal structure, a slower decrease from epochs 10--30 as the representations are refined, and near-convergence from epochs 30--50 where further improvement is marginal. This behavior is consistent with the training dynamics reported by \textcite{chen2020simple} and confirms that 50~epochs are sufficient for convergence on this dataset.

\section{Multi-Threshold Fault Detection Results}\label{sec:multi_threshold_results}

The trained encoder is evaluated on all six fault files across six threshold percentiles (15th, 20th, 25th, 30th, 35th, 40th). Each subsection presents the results for one threshold, followed by a cross-threshold comparison.

\subsection{Results at 15th Percentile Threshold}\label{sec:results_15}

\begin{table}[H]
\centering
\caption{Fault detection results at the 15th percentile threshold. \textit{Time}: inference time per file in seconds.}
\label{tab:results_15}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lcccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{Time (s)} \\
\midrule
Acc gain & -- & -- & -- & -- & -- & -- & -- \\
Acc noise & -- & -- & -- & -- & -- & -- & -- \\
Acc stuck & -- & -- & -- & -- & -- & -- & -- \\
Speed gain & -- & -- & -- & -- & -- & -- & -- \\
Speed noise & -- & -- & -- & -- & -- & -- & -- \\
Speed stuck & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{hil\_detection\_results\_15th.csv} generated by the notebook.}
\end{table}

\subsection{Results at 20th Percentile Threshold}\label{sec:results_20}

\begin{table}[H]
\centering
\caption{Fault detection results at the 20th percentile threshold.}
\label{tab:results_20}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lcccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{Time (s)} \\
\midrule
Acc gain & -- & -- & -- & -- & -- & -- & -- \\
Acc noise & -- & -- & -- & -- & -- & -- & -- \\
Acc stuck & -- & -- & -- & -- & -- & -- & -- \\
Speed gain & -- & -- & -- & -- & -- & -- & -- \\
Speed noise & -- & -- & -- & -- & -- & -- & -- \\
Speed stuck & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{hil\_detection\_results\_20th.csv}.}
\end{table}

\subsection{Results at 25th Percentile Threshold}\label{sec:results_25}

\begin{table}[H]
\centering
\caption{Fault detection results at the 25th percentile threshold.}
\label{tab:results_25}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lcccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{Time (s)} \\
\midrule
Acc gain & -- & -- & -- & -- & -- & -- & -- \\
Acc noise & -- & -- & -- & -- & -- & -- & -- \\
Acc stuck & -- & -- & -- & -- & -- & -- & -- \\
Speed gain & -- & -- & -- & -- & -- & -- & -- \\
Speed noise & -- & -- & -- & -- & -- & -- & -- \\
Speed stuck & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{hil\_detection\_results\_25th.csv}.}
\end{table}

\subsection{Results at 30th Percentile Threshold}\label{sec:results_30}

\begin{table}[H]
\centering
\caption{Fault detection results at the 30th percentile threshold.}
\label{tab:results_30}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lcccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{Time (s)} \\
\midrule
Acc gain & -- & -- & -- & -- & -- & -- & -- \\
Acc noise & -- & -- & -- & -- & -- & -- & -- \\
Acc stuck & -- & -- & -- & -- & -- & -- & -- \\
Speed gain & -- & -- & -- & -- & -- & -- & -- \\
Speed noise & -- & -- & -- & -- & -- & -- & -- \\
Speed stuck & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{hil\_detection\_results\_30th.csv}.}
\end{table}

\subsection{Results at 35th Percentile Threshold}\label{sec:results_35}

\begin{table}[H]
\centering
\caption{Fault detection results at the 35th percentile threshold.}
\label{tab:results_35}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lcccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{Time (s)} \\
\midrule
Acc gain & -- & -- & -- & -- & -- & -- & -- \\
Acc noise & -- & -- & -- & -- & -- & -- & -- \\
Acc stuck & -- & -- & -- & -- & -- & -- & -- \\
Speed gain & -- & -- & -- & -- & -- & -- & -- \\
Speed noise & -- & -- & -- & -- & -- & -- & -- \\
Speed stuck & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{hil\_detection\_results\_35th.csv}.}
\end{table}

\subsection{Results at 40th Percentile Threshold}\label{sec:results_40}

\begin{table}[H]
\centering
\caption{Fault detection results at the 40th percentile threshold.}
\label{tab:results_40}
\renewcommand{\arraystretch}{1.3}
\small
\begin{tabular}{lcccccccc}
\toprule
\textbf{Fault} & \textbf{Windows} & \textbf{Detected} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{Time (s)} \\
\midrule
Acc gain & -- & -- & -- & -- & -- & -- & -- \\
Acc noise & -- & -- & -- & -- & -- & -- & -- \\
Acc stuck & -- & -- & -- & -- & -- & -- & -- \\
Speed gain & -- & -- & -- & -- & -- & -- & -- \\
Speed noise & -- & -- & -- & -- & -- & -- & -- \\
Speed stuck & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{hil\_detection\_results\_40th.csv}.}
\end{table}

\subsection{Cross-Threshold Comparison}\label{sec:cross_threshold}

Table~\ref{tab:threshold_comparison} consolidates the overall detection performance across all six thresholds, enabling direct comparison.

\begin{table}[H]
\centering
\caption{Cross-threshold comparison of overall detection performance.}
\label{tab:threshold_comparison}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcccccc}
\toprule
\textbf{Percentile} & \textbf{Threshold} & \textbf{Avg Prec.} & \textbf{Avg Recall} & \textbf{Avg F1} & \textbf{Avg Acc.} & \textbf{ROC-AUC} \\
\midrule
15th & -- & -- & -- & -- & -- & -- \\
20th & -- & -- & -- & -- & -- & -- \\
25th & -- & -- & -- & -- & -- & -- \\
30th & -- & -- & -- & -- & -- & -- \\
35th & -- & -- & -- & -- & -- & -- \\
40th & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from \texttt{multi\_threshold\_comparison.csv}.}
\end{table}

\begin{figure}[H]
\centering
\fbox{\parbox{0.92\textwidth}{\centering\vspace{4cm}
\textbf{[PLACEHOLDER -- Figure~\ref{fig:metrics_vs_threshold}]}\\[0.5em]
Use: \texttt{part3\_metrics\_vs\_threshold.png} from the notebook.\\
Line plot showing recall, precision, F1, and accuracy (y-axis) vs.\ threshold percentile (x-axis) for all six thresholds.
\vspace{0.5cm}}}
\caption{Detection metrics as a function of threshold percentile. Higher percentiles increase recall at the cost of precision.}
\label{fig:metrics_vs_threshold}
\end{figure}

The cross-threshold comparison reveals the expected trade-off: as the threshold percentile increases from 15 to 40, recall increases (more faults are detected) while precision may decrease (more false positives). The F1-score, which balances both, typically peaks at an intermediate percentile. This analysis provides practical guidance for threshold selection depending on the application's safety requirements.

\subsection{Per-Fault-Type Recall Heatmap}\label{sec:recall_heatmap}

\begin{figure}[H]
\centering
\fbox{\parbox{0.92\textwidth}{\centering\vspace{4.5cm}
\textbf{[PLACEHOLDER -- Figure~\ref{fig:recall_heatmap}]}\\[0.5em]
Use: \texttt{part3\_recall\_heatmap.png} from the notebook.\\
Heatmap with fault types on the y-axis and threshold percentiles on the x-axis, color-coded by recall value.
\vspace{0.5cm}}}
\caption{Per-fault recall heatmap across all six thresholds. Darker color indicates higher recall.}
\label{fig:recall_heatmap}
\end{figure}

The recall heatmap reveals characteristic differences between fault types:

\begin{itemize}
    \item \textbf{Stuck-at faults} typically achieve the highest recall across all thresholds. A frozen signal produces embeddings that are maximally different from normal driving patterns, where both accelerator and speed vary continuously.

    \item \textbf{Noise faults} show moderate recall. The injected noise alters the high-frequency characteristics of the signal, which the CNN encoder detects through its convolutional filters. However, mild noise may overlap with the natural variability of the training data.

    \item \textbf{Gain faults} are generally the hardest to detect. A gain factor close to 1.0 produces signals that remain within the normal amplitude range, differing from healthy data only in subtle proportionality. Detection improves as the gain factor deviates further from unity.
\end{itemize}

\section{Binary Classification: Healthy vs.\ Faulty}\label{sec:binary_results}

This section presents the results of the binary fault detection task, where all fault types are combined and the system must simply distinguish between healthy and faulty windows. This addresses the supervisor's specific request to present the testing results for the ``only fault detection task (only healthy or faulty).''

\subsection{Binary Confusion Matrices}\label{sec:binary_cm}

\begin{figure}[H]
\centering
\fbox{\parbox{0.92\textwidth}{\centering\vspace{5cm}
\textbf{[PLACEHOLDER -- Figure~\ref{fig:confusion_matrices}]}\\[0.5em]
Use: \texttt{part3\_confusion\_matrices.png} from the notebook.\\
Six confusion matrices (one per threshold) showing TN, FP, FN, TP counts for the binary healthy/faulty classification. Arranged in a 2$\times$3 grid.
\vspace{0.5cm}}}
\caption{Binary classification confusion matrices for all six threshold percentiles.}
\label{fig:confusion_matrices}
\end{figure}

\subsection{Binary Classification Metrics}\label{sec:binary_metrics}

Table~\ref{tab:binary_metrics} reports precision, recall, F1-score, and accuracy for the binary classification task across all thresholds.

\begin{table}[H]
\centering
\caption{Binary classification results (healthy vs.\ faulty) across all thresholds. \textit{Time}: total inference time in seconds.}
\label{tab:binary_metrics}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccccccc}
\toprule
\textbf{Pctl.} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Acc.} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} & \textbf{Time (s)} \\
\midrule
15th & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
20th & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
25th & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
30th & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
35th & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
40th & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill from the notebook output. Healthy windows come from the 90\,s calibration set; faulty windows from all 6 fault files.}
\end{table}

\subsection{ROC Curves}\label{sec:roc_results}

\begin{figure}[H]
\centering
\fbox{\parbox{0.92\textwidth}{\centering\vspace{4.5cm}
\textbf{[PLACEHOLDER -- Figure~\ref{fig:roc_curves}]}\\[0.5em]
Use: \texttt{part3\_roc\_curves.png} from the notebook.\\
ROC curve plotting True Positive Rate vs.\ False Positive Rate with the AUC value annotated. Include the diagonal reference line (random classifier).
\vspace{0.5cm}}}
\caption{ROC curves for binary fault detection. The AUC value quantifies the threshold-independent discriminative ability.}
\label{fig:roc_curves}
\end{figure}

The ROC-AUC is a threshold-independent metric and therefore remains constant across the six threshold settings. An AUC significantly above 0.5 confirms that the SSL-learned representations provide genuine discriminative power between healthy and faulty windows, rather than random classification.

\section{Similarity Distribution Analysis}\label{sec:similarity_dist}

\begin{figure}[H]
\centering
\fbox{\parbox{0.92\textwidth}{\centering\vspace{4.5cm}
\textbf{[PLACEHOLDER -- Figure~\ref{fig:similarity_dist}]}\\[0.5em]
Use: \texttt{part3\_similarity\_distributions.png} from the notebook.\\
Overlapping histograms of cosine similarity values for healthy windows (green) and faulty windows (red), with vertical lines marking all six thresholds.
\vspace{0.5cm}}}
\caption{Cosine similarity distributions for healthy and faulty windows, with threshold lines at the 15th through 40th percentiles.}
\label{fig:similarity_dist}
\end{figure}

The similarity distribution plot provides visual insight into the separation between healthy and faulty data in the embedding space. The degree of overlap between the two distributions determines the achievable detection performance: less overlap enables better discrimination. The six threshold lines illustrate how moving the threshold rightward (higher percentile) catches more fault windows but also encroaches on the healthy distribution, increasing false positives.

\section{Computing Cost Analysis}\label{sec:computing_costs}

Table~\ref{tab:computing_costs} presents the measured computing times for each pipeline stage, addressing the supervisor's requirement to discuss computational costs in relation to training and testing time.

\begin{table}[H]
\centering
\caption{Computing cost analysis for all pipeline stages.}
\label{tab:computing_costs}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{llll}
\toprule
\textbf{Phase} & \textbf{Operation} & \textbf{Time (s)} & \textbf{Time (min)} \\
\midrule
\multirow{2}{*}{Part~1} & A2D2 data loading & \textit{(from run)} & -- \\
 & Preprocessing and visualization & \textit{(from run)} & -- \\
\midrule
\multirow{3}{*}{Part~2} & SimCLR training (50 epochs) & \textit{(from run)} & \textit{(from run)} \\
 & \quad of which: forward + backward & \textit{(from run)} & -- \\
 & \quad of which: visualization & \textit{(from run)} & -- \\
\midrule
\multirow{4}{*}{Part~3} & HIL data loading and parsing & \textit{(from run)} & -- \\
 & Embedding extraction (all files) & \textit{(from run)} & -- \\
 & Threshold evaluation (6 thresholds) & \textit{(from run)} & -- \\
 & Visualization and saving & \textit{(from run)} & -- \\
\midrule
\textbf{Total} & \textbf{End-to-end pipeline} & \textit{(from run)} & \textit{(from run)} \\
\bottomrule
\end{tabular}\\[0.3em]
\small\textit{Note: Fill exact values from the notebook timing output. The notebook prints ``PART~X TIME: Y~seconds'' at the end of each part.}
\end{table}

\subsection{Discussion of Computing Costs}\label{sec:cost_discussion}

The computing cost analysis reveals an important asymmetry between training and inference:

\begin{itemize}
    \item \textbf{Training is a one-time cost.} SimCLR training constitutes the majority of the total pipeline time. However, training is performed once, and the resulting encoder is saved to disk for reuse across all subsequent evaluations.

    \item \textbf{Inference is fast.} Embedding extraction for a single window requires only a single forward pass through the encoder (a few milliseconds on CPU, sub-millisecond on GPU). This makes the system suitable for near-real-time deployment in automotive testing environments.

    \item \textbf{Calibration is lightweight.} Computing the healthy centroid and similarity distribution from 88~windows takes less than a second, making deployment to new HIL configurations trivial.

    \item \textbf{Multi-threshold evaluation adds negligible overhead.} Once embeddings and similarities are computed, evaluating against additional thresholds is a simple comparison operation.
\end{itemize}

Compared to the supervised CNN-GRU model of \textcite{ghannoum2025explainable}, which required approximately 23,000~seconds (6.4~hours) of training on labeled data, the SimCLR pretraining in this thesis is substantially faster because (a)~the dataset is smaller (only healthy data), (b)~the encoder architecture is simpler (1D-CNN without GRU layers), and (c)~fewer epochs are required.

\section{Discussion}\label{sec:discussion}

\subsection{Effectiveness of SSL for Automotive Fault Detection}\label{sec:discuss_effectiveness}

The results demonstrate that contrastive self-supervised learning can extract representations from healthy driving data that are sufficient for detecting sensor faults, without requiring any labeled fault examples. The encoder, trained on A2D2 real-world data, produces embeddings that discriminate between healthy and faulty HIL data across all three fault types and both sensor locations.

This validates the core hypothesis of the thesis: normal sensor behavior exhibits regularities that can be captured in a learned embedding space, and sensor faults produce deviations from these regularities that are detectable through similarity-based scoring.

\subsection{Cross-Domain Transfer}\label{sec:discuss_transfer}

A particularly noteworthy finding is the successful transfer from A2D2 (real vehicle) to HIL (simulation). Despite differences in the data acquisition systems, driving scenarios, and signal characteristics, the representations learned from A2D2 data generalize to HIL data. Two factors enable this transfer:

\begin{enumerate}
    \item \textbf{Consistent normalization:} The Z-score scaler fitted on A2D2 data is applied to HIL data, projecting both domains into a comparable feature space.
    \item \textbf{Domain-invariant features:} The contrastive learning objective encourages the encoder to capture fundamental signal dynamics (e.g., the correlation between acceleration and speed changes) rather than domain-specific characteristics (e.g., absolute amplitude ranges).
\end{enumerate}

\subsection{Impact of Threshold Selection}\label{sec:discuss_threshold}

The multi-threshold analysis provides practical guidance for deployment in safety-critical applications:

\begin{itemize}
    \item For \textbf{high-ASIL applications} (e.g., ASIL-C/D), where missing a fault is unacceptable, higher percentile thresholds (35th--40th) should be used to maximize recall. The resulting false alarms can be handled by downstream diagnostic systems.

    \item For \textbf{low-ASIL applications} (e.g., ASIL-A/B), where false alarms are disruptive, lower percentile thresholds (15th--20th) should be used to maximize precision. Only high-confidence detections are reported.

    \item The \textbf{optimal balanced threshold} corresponds to the percentile with the highest F1-score, which balances the competing demands of precision and recall.
\end{itemize}

\subsection{Fault Type Detectability}\label{sec:discuss_fault_types}

The per-fault analysis reveals that detectability is strongly correlated with the physical severity of the fault:

\begin{itemize}
    \item \textbf{Stuck-at faults} are the most detectable because a frozen signal violates the temporal dynamics that the encoder has learned to expect from healthy driving (continuous variation in both pedal position and speed).

    \item \textbf{Noise faults} are moderately detectable. The added noise introduces high-frequency components that are absent in the smooth healthy signals, but mild noise may be confused with the jittering augmentation used during training.

    \item \textbf{Gain faults} are the least detectable when the gain factor is close to 1.0, because the signal shape remains correct---only the amplitude is scaled. This is physically realistic: a 5\% gain error in a pedal position sensor is genuinely difficult to detect without reference measurements.
\end{itemize}

This magnitude-dependent detectability is a \emph{strength} of the approach, as it reflects the physical reality that more severe faults are inherently more distinguishable from normal operation.

\subsection{Limitations}\label{sec:limitations}

\begin{enumerate}
    \item \textbf{Two-sensor limitation.} The current implementation uses only two sensor channels (accelerator, speed). Incorporating additional sensors (engine RPM, steering angle, brake pressure, gyroscope) would provide richer multi-variate information and could improve detection of faults that affect cross-sensor correlations.

    \item \textbf{Window-level granularity.} Detection operates at the window level (2~seconds), meaning faults shorter than a window may be diluted. Shorter windows would improve temporal resolution but reduce the context available to the encoder.

    \item \textbf{No fault localization.} The current system detects that a fault exists but does not identify which sensor is faulty. Ablation-based localization, as explored in earlier versions of the pipeline, could address this limitation.

    \item \textbf{Threshold dependency.} Detection performance varies with threshold selection, and there is no single optimal threshold for all fault types and severities. An adaptive thresholding mechanism could improve robustness.
\end{enumerate}
